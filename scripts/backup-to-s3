#!/usr/bin/env bash

# *This file is meant to be run as root or with sudo*
# It will transfer local backups of clickhouse data and postgres to an s3 bucket
# It requires root or sudo to write logs
# The script is intended to be run as a cron job

# confirm that use is root or sudo, if not exit
if [ "$EUID" -ne 0 ]
  then echo "Please run as root or with sudo"
  exit
fi

# Get the directory of the script
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
# Get the dir 1 level up
DIR="$(dirname "$DIR")"

# Load the environment variables
source "$DIR/.env"

REQUIRED_VARS=(
  LOCAL_BACKUP_PATH
  LOCAL_POSTGRES_BACKUP_PATH
  LOCAL_CLICKHOUSE_BACKUP_PATH
  LOCAL_BACKUP_RETENTION_DAYS
  S3_BACKUP_RETENTION_DAYS
  S3_BUCKET_NAME
  S3_BUCKET_REGION
  S3_ACCESS_KEY_ID
  S3_SECRET_ACCESS_KEY
  LOGS_PATH
)

for var in "${REQUIRED_VARS[@]}"; do
  if [ -z "${!var}" ] || [ "${!var}" = "REPLACE_ME" ]; then
    echo "$var is unset. Please set this in your .env file." 
    exit 1
  fi
done

# set backup paths
S3_CLICKHOUSE_PATH="s3://${S3_BUCKET_NAME}/plausible/clickhouse-data/"
S3_POSTGRES_PATH="s3://${S3_BUCKET_NAME}/plausible/postgres-data/"

# log file
LOG_FILE="${LOGS_PATH}plausible-backup-to-s3.log"

# function to log messages
log_message() {
  local log_dir=$(dirname "$LOG_FILE")
  local log_file=$(basename "$LOG_FILE")
  local timestamp=$(date +"%Y-%m-%d %H:%M:%S")
  local message="${1//[$'\n']/}" # Remove newline characters from the message
  local level="${2:-INFO}" # Optional log level

  # Create the log directory if it doesn't exist
  if [ ! -d "$log_dir" ]; then
    mkdir -p "$log_dir"
  fi

  # Create the log file if it doesn't exist
  if [ ! -f "$LOG_FILE" ]; then
    touch "$LOG_FILE"
  fi

  echo "$timestamp - [$level] $message" >> "$LOG_FILE"
}

# make our credentials available to aws cli
export AWS_ACCESS_KEY_ID=$S3_ACCESS_KEY_ID
export AWS_SECRET_ACCESS_KEY=$S3_SECRET_ACCESS_KEY
export AWS_DEFAULT_OUTPUT="table"

sync_to_s3() {
  local service="$1"
  
  
  if [ "$service" == "clickhouse" ]; then
    local local_backups_dir="$LOCAL_BACKUP_PATH$LOCAL_CLICKHOUSE_PATH"
    local s3_path="$S3_CLICKHOUSE_PATH"
    local included_files="*.zip"
  fi

  if [ "$service" == "postgres" ]; then
    local local_backups_dir="$LOCAL_BACKUP_PATH$LOCAL_POSTGRES_PATH"
    local s3_path="$S3_POSTGRES_PATH"
    local included_files="*.tar"
  fi

  log_message "Starting $service backup sync to S3..."
  output=$(aws s3 sync $local_backups_dir $s3_path --exclude "*" --include $included_files --region ${S3_BUCKET_REGION} 2>&1)

  local transfer_total_files=$(echo "$output" | awk '/upload:/ {count++} END {printf "%d\n", count}')

  if [ $? -eq 0 ]; then
    log_message "$service backup sync completed successfully"
    log_message "total files transferred: $transfer_total_files"
  else
    log_message "$service backup sync failed." "ERROR"
    log_message "$output" "ERROR"
  fi
}

prune_s3_backups() {
  local service="$1"
  local s3_path="$2"
  
  log_message "Pruning old $service backups from S3 (retention: ${S3_BACKUP_RETENTION_DAYS} days)..."
  
  local cutoff_date=$(date -d "-${S3_BACKUP_RETENTION_DAYS} days" +%s 2>/dev/null || date -v-${S3_BACKUP_RETENTION_DAYS}d +%s)
  local deleted_count=0
  
  aws s3 ls $s3_path --region ${S3_BUCKET_REGION} | while read -r line; do
    local file_date=$(echo $line | awk '{print $1" "$2}')
    local file_name=$(echo $line | awk '{print $4}')
    
    if [ -n "$file_name" ]; then
      local file_timestamp=$(date -d "$file_date" +%s 2>/dev/null || date -j -f "%Y-%m-%d %H:%M:%S" "$file_date" +%s)
      
      if [ $file_timestamp -lt $cutoff_date ]; then
        log_message "Deleting old backup: $file_name"
        aws s3 rm "${s3_path}${file_name}" --region ${S3_BUCKET_REGION}
        ((deleted_count++))
      fi
    fi
  done
  
  log_message "Pruned $deleted_count old $service backup(s) from S3"
}

sync_to_s3 "clickhouse"
prune_s3_backups "clickhouse" "$S3_CLICKHOUSE_PATH"

sync_to_s3 "postgres"
prune_s3_backups "postgres" "$S3_POSTGRES_PATH"
